{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Kernel Hearding for collecting super samples of a distribution\n",
    "\n",
    "## The problem(s)\n",
    "Deterministic methods for numerically calculating mulit-dimensional integrals $I = \\int f(x)p(x)dx$ is scale poorly as the number of dimensions increase. This problem occurs frequently in calculating expectation values and marginal distributions.\n",
    "\n",
    "Random iid sampling of $p(x)$ and evaluating the integral as $I \\approx \\frac{1}{N} \\sum f(x_i)$ scales in a dimension agnostic way and converges to the true value as $I\\approx \\frac{1}{\\sqrt(N)}$. \n",
    "\n",
    "A second problem is the need to reduce the amount of memory required estimate $p(x)$ for future use. An algorithm that can better estimate $p(x)$ would help solve both of these problems.\n",
    "\n",
    "Improving this rate of convergence is a goal of kernel herding which seeks take advantage of the assumption that $p(x)$ is smooth and sample it in an optimal way to produce a set of 'super samples' that\n",
    "\n",
    "## Kernel herding\n",
    "A newer method, kernel herding, promises to converge on an estimate of $I$ somewhere between $\\frac{1}{\\sqrt(N)}$ and $\\frac{1}{N}$. Kernel herding assumes that $p(x)$ is 'smooth', and then detrministically and recursively picking samples from $p(x)$. Every new sample 'knows' about the previous samples and stays away from them while still faithfully representing $p(x)$.\n",
    "\n",
    "This algorithm finds each 'super sample' $x_{ss,i}$ by finding the point that maximizes:\n",
    "$x_{ss,i} = argmax_x(\\int k(x,x')p(x')dx' - \\frac{1}{i}\\sum k(x,x_{ss,i-1})$)\n",
    "where $k(.,.)$ is a suitable kernel. For this demonstration I use $k(x,x')=e^{-norm(x-x')/\\gamma^2}$ with $\\gamma = 1$.\n",
    "\n",
    "I call the first term the expectation value of the kernel expKer = $\\int k(x,x')p(x')dx'$ and can be viewed as a weight drawing a point near regions of high probability density, while the second term sumKer = $\\frac{1}{i}\\sum k(x,x_{ss,i-1})$ acts to repel each point from all previous points.\n",
    "\n",
    "## Implementation here\n",
    "Create a low dimensional gaussian mixture model.\n",
    "\n",
    "Sample points from $p(x)$ as an estimate.\n",
    "\n",
    "Select an arbitrary start point, I pick the origin.\n",
    "\n",
    "Estimate the first term expKer = $\\int k(x,x')p(x')dx' \\approx \\sum_j e^{-norm(x-x_j)^2/\\gamma^2}$\n",
    "\n",
    "Calculate the second term sumKer = $\\sum_{ss,i} e^{-norm(x-x_{ss,i})^2/\\gamma^2}$\n",
    "\n",
    "Use gradient descent to find argmax\n",
    "\n",
    "Repeat for $N$ points\n",
    "\n",
    "\n",
    "## Tradeoffs and comments\n",
    "For creating a set of super samples that optimally reduce the amount of memory needed to store a distribution, herding is a good idea as long as we can afford the calculation of the samples.\n",
    "\n",
    "Each super sample needs an integration and optimization so I'm trading an iid Monte Carlo sampling integral calculation for another. This means the calculation of expKer needs to be much less costly than the sampling $f(x)$. Monte Carlo sampling is a much better idea when sampling $f(x)$ is cheap.\n",
    "\n",
    "Gradient descent, as implemented here, isn't always robust.\n",
    "\n",
    "Each calculation of $x_{ss,i}$ is an infinite memory process and required knowledge of all supersamples found so far, meaning paralellization is more difficult (impossible?) than uncorrelated Monte Carlo sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Python the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib pandas pypr scipy mpld3 scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd #to plot the GMM\n",
    "from scipy.optimize import minimize #to do grad descent to fing argmax\n",
    "import time #time each iteration\n",
    "%matplotlib inline\n",
    "import mpld3 #for plot tools\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create, sample, and plot GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "\n",
    "def sampleFromGauss(numDim, numGauss, muScale, covScale, numSamples):\n",
    "\n",
    "  # Initialize variables\n",
    "  mu = muScale * (np.random.rand(numGauss, numDim) - 0.5)\n",
    "  sigma = np.zeros((numGauss, numDim, numDim))\n",
    "  for i in range(numGauss):\n",
    "    cov = covScale * (np.random.rand(numDim, numDim) - 0.5 + 0.7 * np.identity(numDim))\n",
    "    sigma[i] = np.dot(cov, cov.T)\n",
    "\n",
    "  # Create Gaussian Mixture Model\n",
    "  gmm = GaussianMixture(n_components=numGauss, covariance_type='full')\n",
    "  gmm.means_ = mu\n",
    "  gmm.covariances_ = sigma\n",
    "  gmm.weights_ = np.ones(numGauss) / numGauss\n",
    "  gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma))\n",
    "\n",
    "# Sample from GMM\n",
    "  samples, _ = gmm.sample(numSamples)\n",
    "  return samples, (gmm.weights_, mu, sigma), gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate herding samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def expKer(x, samples, gamma):\n",
    "    \"\"\"\n",
    "    Calculates the expectation value of the exponential kernel to find argmax_x.\n",
    "    Parameters:\n",
    "    x (numpy.ndarray): Candidate super sample to optimize.\n",
    "    samples (numpy.ndarray): The Gaussian Mixture Model (GMM) samples.\n",
    "    gamma (float): Kernel hyperparameter, always 1 for the demo.\n",
    "    Returns:\n",
    "    float: The expectation value of the exponential kernel.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    numSamples = samples.shape[0]\n",
    "    k = np.exp(-np.linalg.norm(x - samples, axis=1) / gamma**2)\n",
    "    exp_est = np.mean(k)\n",
    "    return exp_est\n",
    "\n",
    "def sumKer(x,xss,numSSsoFar,gamma):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the sum of the kernel function k(x, x_ss) for the given number of super samples so far.\n",
    "    Parameters:\n",
    "    x (numpy.ndarray): The candidate super sample to optimize.\n",
    "    xss (numpy.ndarray): The array of super samples.\n",
    "    numSSsoFar (int): The number of super samples so far.\n",
    "    gamma (float): The kernel hyperparameter.\n",
    "    Returns:\n",
    "    float: The normalized sum of the kernel values.\n",
    "    \"\"\"    \n",
    "    # Vectorized calculation of the sum of kernels\n",
    "    k = np.exp(-np.linalg.norm(x - xss[:numSSsoFar], axis=1) / gamma**2)\n",
    "    total = np.sum(k)\n",
    "    s = total / (numSSsoFar + 1)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create samples\n",
    "np.random.seed([1])\n",
    "samples, coeffs = sampleFromGauss(numDim=3, numGauss=4, muScale=5, covScale=1, numSamples=int(1e3))\n",
    "#plot them for inspection\n",
    "df = pd.DataFrame(samples)\n",
    "pd.plotting.scatter_matrix(df, alpha=0.2, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-- main function\n",
    "\n",
    "def herd(samples,totalSS,gamma):\n",
    "    #-- calculate totalSS super samples from the distribution estimated by samples with kernel hyperparam gamma\n",
    "    \n",
    "    #init vars and extract useful info from inputs\n",
    "    #get GMM dims and num samples\n",
    "    numDim = samples.shape[1]\n",
    "    numSamples = samples.shape[0]\n",
    "    \n",
    "    #init vars\n",
    "    gradientFail = 0; #count when optimization fails, debugging\n",
    "    xss = np.zeros((totalSS,numDim)) #open space in mem for array of super samples\n",
    "    i=1\n",
    "    #gradient descent can have some probems, so make bounds to terminate if goes too far away\n",
    "    minBound = np.min(samples)\n",
    "    maxBound = np.max(samples)\n",
    "    #start our search at the origin, could be a random point\n",
    "    bestSeed = np.zeros(numDim)\n",
    "    \n",
    "#     tick = time.clock()\n",
    "    while i<totalSS:\n",
    "        # print('.'),\n",
    "        #debugging stuff\n",
    "        #print \"Working on SS num ber i=%d\" % i\n",
    "        #build function for gradient descent to find best point\n",
    "        f = lambda x: -expKer(x,samples,gamma)+sumKer(x,xss,i,gamma)\n",
    "        results = minimize(f,\n",
    "                   bestSeed,\n",
    "                   method='L-BFGS-B',\n",
    "                   bounds=[(minBound, maxBound)] * numDim,\n",
    "                   options={'ftol': 1e-9, 'disp': False})\n",
    "#         print \"results.x\"\n",
    "#         print results.x\n",
    "        \n",
    "        #if grad descent failed, pick a random sample and try again\n",
    "        if np.min(results.x) < minBound or np.max(results.x) > maxBound:\n",
    "            bestSeed=samples[np.random.choice(numSamples)]\n",
    "            gradientFail=gradientFail+1\n",
    "#             print \"Gradient descent failed..............\"\n",
    "            continue\n",
    "        \n",
    "        #pick next best start point to start minimization, this is how Chen, Welling, Smola do it\n",
    "        #find best super sample that maximizes argmax and use that as a seed for the next search\n",
    "        #init or clear seed array\n",
    "        seed=np.array([])\n",
    "        for j in range(i):\n",
    "            seed = np.append(seed,-expKer(xss[j,:],samples,gamma)+sumKer(xss[j,:],xss,i,gamma)) # put f function values of xss's in seed array\n",
    "        bestSeedIdx = np.argmin(seed)\n",
    "        bestSeed=xss[bestSeedIdx,:]\n",
    "        \n",
    "        #grad descent succeeded (yay!), so assign new value to super samples\n",
    "        xss[i,:]=results.x\n",
    "        \n",
    "        i=i+1\n",
    "        # toc = time.clock()\n",
    "#         print \"Time elapsed %d\" % (toc-tick)\n",
    "    return xss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def plot_gmm_density(coeffs, ax=None):\n",
    "    weights, means, covariances = coeffs\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    x = np.linspace(-10, 10, 200)  # Increase the number of points\n",
    "    y = np.linspace(-10, 10, 200)  # Increase the number of points\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "    Z = np.zeros((200, 200))  # Adjust the shape accordingly\n",
    "    for weight, mean, cov in zip(weights, means, covariances):\n",
    "        rv = multivariate_normal(mean[:2], cov[:2, :2])\n",
    "        Z += weight * rv.pdf(XX).reshape(200, 200)\n",
    "    \n",
    "    ax.contourf(X, Y, Z, levels=100, cmap='Blues', norm=LogNorm())\n",
    "    ax.set_xlim(-5,5)\n",
    "    ax.set_ylim(-5,5)\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.title('Gaussian Mixture Model Density')\n",
    "    plt.colorbar(ax.contourf(X, Y, Z, levels=50, cmap='viridis', norm=LogNorm()))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalSS=100\n",
    "xss = herd(samples,totalSS,gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot herd samples\n",
    "\n",
    "\n",
    "# Plot random samples\n",
    "ax.plot(samples[:, 0], samples[:, 1], '.', label='Random Samples')\n",
    "ax.plot(xss[:, 0], xss[:, 1], 'o', label='Herd Samples')\n",
    "# Plot the GT GMM showing the density with colors\n",
    "plot_gmm_density(coeffs, ax=ax)\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot first 2 dims of distribution and super samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I follow Chen, Welling, and Smola, and report the error as defined by equation 5:$err_N = norm(\\mu_p - \\frac{1}{N}\\sum_i x_{ss,i})$. Essentially this compares the mean of the GMM to the mean estimated by super sampling. I compare this to the iid sampled mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate herding error\n",
    "mu_p = np.mean(samples,axis=0)\n",
    "err=np.zeros(totalSS)\n",
    "for i in range(totalSS):\n",
    "    err[i]    = np.linalg.norm(mu_p-np.sum(xss[1:i,:]        ,axis=0)/i)\n",
    "\n",
    "idx = np.random.choice(1000,totalSS)\n",
    "samples_iid=samples[idx,:]\n",
    "err_iid=np.zeros(totalSS)\n",
    "for i in range(totalSS):\n",
    "    err_iid[i]= np.linalg.norm(mu_p-np.sum(samples_iid[1:i,:],axis=0)/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(err,'-o')\n",
    "plt.plot(err_iid)\n",
    "plt.plot([1,totalSS],[1,1./totalSS],'--')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend(['Herding error','iid error','~1/N'], loc='lower left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernelherding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
